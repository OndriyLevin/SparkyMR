# Описание архитектуры и плана реализации

**Цель:** Создать ИИ-помощника для смешанной реальности (наподобие Призрака из Destiny) для платформы Meta Quest с использованием Meta Spatial SDK. Помощник будет визуально представлен как 3D-сфера, которую можно будет вызывать и закреплять в пространстве с помощью жестов рук. Он сможет разговаривать с пользователем, используя цепочку: Голос -> Текст -> Gemini -> Текст -> Синтез речи. Если будет разрешено, он также будет обрабатывать контекст окружения через данные камеры.

## Требует внимания / Обсуждения
> [!IMPORTANT]
> **Доступ к камере в MR (Смешанной Реальности):** Доступ к сырым кадрам камеры на Quest в потребительских приложениях ограничен по соображениям конфиденциальности, если только это не 2D-приложение (панель) или приложение со специальным одобрением от Meta. Telegram, вероятно, использует либо 2D-Android режим, либо запрашивает специальные системные разрешения. Мы начнем разработку с 3D-пространственного приложения и попробуем использовать стандартные API `android.hardware.camera2` или `CameraX`. Если Quest заблокирует чистый видеопоток в иммерсивном режиме, мы будем искать обходные пути: например, использование Scene Understanding (понимание сцены), либо запуск приложения как 2D-панели (которая умеет рендерить 3D за своими пределами).

> [!NOTE]
> **Язык и стек проекта:** Мы будем разрабатывать приложение, используя чистый **Android Studio**, **Kotlin** и **Meta Spatial SDK**. Нам не потребуются тяжелые движки вроде Unity или Unreal, что позволит сохранить приложение легким, быстрым и более нативным для операционной системы Quest.

## Предлагаемые изменения и архитектура
Мы создадим новую структуру Android-проекта.

### 1. Настройка ядра приложения
- Инициализировать Gradle-проект с поддержкой Kotlin.
- Интегрировать библиотеки `com.meta.spatial:core` и `com.meta.spatial:toolkit`.
- Настроить `AndroidManifest.xml` с нужными флагами: `android.hardware.vr.headtracking`, `oculus.software.handtracking`, и разрешениями: `android.permission.RECORD_AUDIO`, `android.permission.CAMERA` и `android.permission.INTERNET`.

### 2. Компоненты системы

#### UI и Представление (Пространственный Рендеринг)
Мы создадим `MainActivity`, наследующую специальные классы из Spatial SDK (например, `SpatialActivity`).
- Помощник будет представлять собой `Mesh.createSphere()` (либо загруженную GLTF-модель сферы).
- Мы добавим к ней светящийся материал (glowing material), чтобы она была похожа на ИИ.

#### Менеджер взаимодействий (Жесты)
- Система будет считывать данные отслеживания рук (Hand Tracking).
- Жест `Щипок (Pinch)` будет использоваться для призыва сферы к руке или к лицу пользователя.
- Жест `Открытая ладонь (Open Palm)` будет использоваться для закрепления сферы в текущих мировых координатах (чтобы она висела в воздухе).
- Код будет интерполировать (сглаживать) движение сферы, чтобы она плавно подлетала.

#### Ядро Аудио и ИИ (Gemini)
- Извлекать звук через стандартный Android-класс `AudioRecord`.
- Использовать встроенный или внешний TTS-движок (Text-to-Speech) для озвучивания ответов.
- Реализовать HTTP/REST-клиент (например, используя Retrofit или Ktor) либо официальный Gemini Android SDK для связи с ИИ.
- Поток данных: Микрофон -> Текст (STT или напрямую аудио в Gemini 1.5) -> Gemini -> Ответный текст -> TTS-движок.

## План тестирования и проверки
### Ручная проверка
1. **Деплой:** Собрать APK и загрузить его на гарнитуру Meta Quest (через SideQuest или ADB).
2. **Визуал:** Убедиться, что приложение запускается в иммерсивном режиме и включен сквозной вид (Passthrough) реальной комнаты.
3. **Жесты:** Выполнить жест вызова и убедиться, что сфера появляется рядом. Сделать жест "отпускания" и убедиться, что она остается на месте в пространстве комнаты.
4. **Общение с ИИ:** Сказать фразу в микрофон шлема и дождаться озвученного, связного ответа от Gemini API.
5. **Прототип зрения:** Попросить помощника "Что я сейчас вижу?" для запуска экспериментального захвата изображения с камер и проверить стабильность ответа.
