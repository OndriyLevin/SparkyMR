# Задачи по разработке SparkyMR

Этот документ отслеживает прогресс создания ИИ-помощника для смешанной реальности на платформе Meta Quest.

- [ ] **Фаза 1: Настройка проекта и базовый пространственный рендеринг**
  - [ ] Инициализировать проект Android Studio с зависимостями Meta Spatial SDK.
  - [ ] Настроить `AndroidManifest.xml` для работы в Immerisve VR/MR, отслеживания рук и доступа к микрофону.
  - [ ] Отрисовать базовую 3D-сферу в пространстве смешанной реальности с помощью Spatial SDK.

- [ ] **Фаза 2: Отслеживание рук и жесты**
  - [ ] Реализовать логику отслеживания рук пользователя.
  - [ ] Реализовать жест **"Вызов"**: Определять специфический жест (например, щипок или поворот ладони к себе), чтобы сфера прилетала к руке или повисала перед глазами.
  - [ ] Реализовать жест **"Закрепление/Сброс"**: Определять жест отпускания (например, открытая ладонь), чтобы оставить сферу по фиксированным координатам в комнате.

- [ ] **Фаза 3: Звук и голосовой интерфейс**
  - [ ] Запрашивать разрешения на запись звука во время работы приложения (Runtime permissions).
  - [ ] Реализовать запись звука с микрофона Quest (через `AudioRecord`).
  - [ ] Реализовать синтез речи (Text-to-Speech, TTS), чтобы сфера могла отвечать голосом (используя нативный Android TTS или внешний сервис вроде ElevenLabs/OpenAI).

- [ ] **Фаза 4: Интеграция ИИ Gemini**
  - [ ] Подключить Google Gemini API (с использованием официального Kotlin SDK или HTTP-клиента).
  - [ ] Отправлять записанные голосовые команды (или распознанный текст) в модель Gemini.
  - [ ] Получать текстовые ответы и озвучивать их через TTS.

- [ ] **Фаза 5: Зрение и контекст (Доступ к камере)**
  - [ ] Исследовать и реализовать доступ к API Camera2 / CameraX в иммерсивном окружении Quest (или найти рабочие обходные пути).
  - [ ] Захватывать периодические кадры или скриншоты по запросу пользователя.
  - [ ] Отправлять визуальные данные вместе с голосовым вопросом в Gemini API для реализации функции "посмотри на это".
